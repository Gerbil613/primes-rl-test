'''
TODO
2. Normalize perturbations for when attack=1 so that the differential is within delta
3. Make autogenerated MDP's rewards path-based rather than edge-based so that long paths don't have advantage
4. Mayuri says not to manually connected nodes that happen to be disconnected, just discard them
'''
import numpy as np
from copy import deepcopy
import matplotlib.pyplot as plt
from path import Path
from mdp import MDP

mdp = MDP()
path_to_corrupt = None

initial_estimations = None
estimations = None

p = 1
delta = 1

def determine_path_to_corrupt():
    '''determine_path_to_corrupt() -> Path
    outputs path in MDP that is best for adversary to corrupt'''
    P_p = mdp.P_star
    for P_i in mdp.paths: # don't iterate over best path
        if P_i.reward == mdp.P_star.reward: continue
        b_i = mdp.traversal_factors[mdp.P_star.states[-1]] + mdp.traversal_factors[P_i.states[-1]] # sum traversal factors of final state in each path
        for P_j in mdp.paths:
            if P_j.reward == P_i.reward or P_j.reward == mdp.P_star.reward: continue

            opt = 99999
            for state in P_j:
                if (state in P_i) != (state in mdp.P_star) and mdp.traversal_factors[state] < opt and state != mdp.start:
                    opt = mdp.traversal_factors[state]
            
            b_i += 1/opt

        if P_i.reward < P_p.reward and mdp.P_star.reward - b_i*p*delta < P_i.reward:
            P_p = P_i

    return P_p

def learn(epsilon, num_warm_episodes, attack=0, verbose=1, graph=True, lw=5, num_epochs=500, num_episodes=200):
    '''learn(int, float, int) -> None
    trains global variable "values" to learn Q-values of maze'''
    global estimations, initial_estimations

    label = "Greedy Victim, " if epsilon < 1 else "Random Victim, "
    label += ("Baseline Adversary, " if attack == 1 else "Dynamic Adversary, ") if attack > 0 else "No Adversary, "
    label += "Warm Start" if num_warm_episodes > 0 else "No Warm Start"

    if verbose > 0:
        print("COMMENCING TRAINING PROTOCOL\nNumber of Epochs: " + str(num_epochs) + "\nNumber of Episodes per Epoch: " + str(num_episodes))

    performance_history = np.zeros((num_epochs, num_episodes))

    for epoch in range(num_epochs):
        estimations = np.zeros((len(mdp.states))) * 9
        times_visited = np.zeros((len(mdp.states)))

        for episode in range(num_episodes + num_warm_episodes):
            path = best_path(epsilon if episode >= num_warm_episodes else 1, estimations)
            has_attacked = False
            can_attack = np.random.random() <= p
            differential = 0
            for index in range(1,len(path.states)):
                new_state = path.states[index]
                times_visited[new_state] += 1
                reward = mdp.sample_reward_function(new_state)
                original_reward = reward
                
                if attack == 1 and can_attack:
                    if new_state in mdp.P_star:
                        reward -= delta/float(len(path.states) - 1) # subtract 1 to ensure delta bound; if a path contains n states, it contains n-1 edges

                    else:
                        reward += delta/float(len(path.states) - 1)
                    has_attacked = True

                if attack == 2 and can_attack and not has_attacked:
                    if new_state == path_to_corrupt.states[-1]:
                        reward += delta
                        has_attacked = True

                    elif new_state == mdp.P_star.states[-1]:
                        reward -= delta
                        has_attacked = True

                    elif new_state in path_to_corrupt and new_state not in mdp.P_star and path.reward != mdp.P_star.reward and path.reward != path_to_corrupt.reward:
                        has_attacked = True
                        reward += delta

                    elif new_state not in path_to_corrupt and new_state in mdp.P_star and path.reward != mdp.P_star.reward and path.reward != path_to_corrupt.reward:
                        reward -= delta
                        has_attacked = True
                
                differential += abs(reward - original_reward)
                n = float(times_visited[new_state]) # TODO: make sure for baseline adversary that delta doesn't get exceeded
                estimations[new_state] = reward / n + estimations[new_state] * (n-1)/n

            assert differential <= delta

            if graph and episode >= num_warm_episodes:
                performance_history[epoch][episode - num_warm_episodes] = evaluate(estimations)

    if graph:
        plt.plot(range(num_episodes), np.average(performance_history, axis=0), label=label, alpha=0.5, lw=lw)

def evaluate(estimations):
    '''evaluate() -> int
    evaluates the global var "values" according to a deterministic (non-epsilon) greedy policy'''
    return best_path(0, estimations).reward

def best_path(epsilon, estimations):
    '''best_path(int, arr) -> path
    with probability epsilon, outputs random path in mdp
    with probability 1-epsilon, outputs path whose reward is best according to our estimations'''
    if np.random.random() < epsilon:
        return np.random.choice(mdp.paths)

    best_paths = [] # if multiple paths are estimated to have identical rewards and are also best, we choose randomly from that set
    best_reward = -9999
    for path in mdp.paths:
        estimated_reward = 0
        for state in path:
            estimated_reward += estimations[state]

        if estimated_reward > best_reward:
            best_reward = estimated_reward
            best_paths = [path]

        elif estimated_reward == best_reward:
            best_paths.append(path)
    
    return np.random.choice(best_paths)

def main():
    global path_to_corrupt
    mdp.load_maze('mazes/testmaze.txt')
    #mdp.load_random(8, p_edge=0.75)
    path_to_corrupt = determine_path_to_corrupt()
    print([str(x) for x in mdp.paths])
    print('Number of paths:', len(mdp.paths))
    print('Path to switch:', path_to_corrupt)
    print('Optimal path:', mdp.P_star)
    attack = 1
    learn(0.3, 0, attack=attack, lw=1)
    learn(0.3, 50, attack=attack, lw=2)
    learn(1, 0, attack=attack, lw=3)
    learn(1, 50, attack=attack, lw=4)

    plt.legend(loc="lower right")
    plt.xlabel('Episode')
    plt.ylabel('Performance (Final Reward)')
    plt.show()

if __name__ == '__main__':
    main()